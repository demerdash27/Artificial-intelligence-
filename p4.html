<!DOCTYPE html>
<html>
<head>
	<title>Artificial intelligence
</title>
</head>
<body>
	<h1>Artificial intelligence
</h1>
	<h2>links</h2>
	<ul>
  <li><a href="index.html">main page</a></li>
  <li><a href="p1.html">Introduction</a></li>
  <li><a href="p2.html">table</a></li>
   <li><a href="p3.html">image</a></li>
  <li><a href="p4.html">applications</a></li>
</ul>
Geometric representation learning has recently shown great promise in several machine learning settings, ranging from relational learning to language processing and generative models. In this work, we consider the problem of performing manifold-valued regression onto an hyperbolic space as an intermediate component for a number of relevant machine learning applications. In particular, by formulating the problem of predicting nodes of a tree as a manifold regression task in the hyperbolic space, we propose a novel perspective on two challenging tasks: 1) hierarchical classification via label embeddings and 2) taxonomy extension of hyperbolic representations. To address the regression problem we consider previous methods as well as proposing two novel approaches that are computationally more advantageous: a parametric deep learning model that is informed by the geodesics of the target space and a non-parametric kernel-method for which we also prove excess risk bounds. Our experiments show that the strategy of leveraging the hyperbolic geometry is promising. In particular, in the taxonomy expansion setting, we find that the hyperbolic-based estimators significantly outperform methods performing regression in the ambient Euclidean space.

</body>
</html>